1. Cookies

  A. What is a cookie in web development terms?
  -- Cookies are text files with small pieces of data — like a username and password — that are used to identify your computer as you use a 
     computer network. Specific cookies known as HTTP cookies are used to identify specific users and improve your web browsing experience.
     or
     Cookies are client-side files that contain user information. Cookie ends depending on the lifetime you set for it. You don't need to start
     cookie as it is stored in your local machine. The official maximum cookie size is 4KB. A cookie is not dependent on Session.
    
  -- https://www.geeksforgeeks.org/cookies-used-website/
     
 
2. Session

  A. What is a session?
  -- Sessions are server-side files which contain user information. A session ends when a user closes his browser. A session is dependent on Cookie.
     Within-session you can store as much data as you like. The only limits you can reach is the maximum memory a script can consume at one time,
     which is 128MB by default.
     
  B. How a server maintains a session?
  -- Sessions are maintained automatically by a session cookie that is sent to the client when the session is first created. The session cookie
     contains the session ID, which identifies the client to the browser on each successive interaction.    
   
   
3. What is a sitemap?
  -- A sitemap is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them.
     Search engines like Google read this file to crawl your site more efficiently.
     
  Purpose: A Sitemap is an XML file that lists the URLs for a site. It allows webmasters to include additional information about each URL: when it
           was last updated, how often it changes, and how important it is in relation to other URLs of the site.


4. What is a robots.txt file in a website and why it is needed?
  -- A robots. txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your 
     site with requests; it is not a mechanism for keeping a web page out of Google.
     
  -- The robots. txt file controls which pages are accessed. The robots meta tag controls whether a page is indexed, but to see this tag the page 
     needs to be crawled. If crawling a page is problematic (for example, if the page causes a high load on the server), you should use the robots.
     
